# -*- coding: utf-8 -*-
"""Hybrid_clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-w88xQ1exkOl_Dp6q_jFzIizZ3LT28_m
"""

#Loading the dataset
import pandas as pd
df=pd.read_csv("/content/Final_dataset.csv")
df.head()

#dataframe information
df.info()
df.describe()

#Data cleaning
#1)Handling missing values
#Here we drop those columns that have more than 90% values as null
a=len(df)
threshold=a*0.90
columns_to_drop=df.columns[df.isnull().sum()>threshold]
print((columns_to_drop))
df.drop(columns=columns_to_drop,inplace=True)

#2)removing duplicates rows
df.drop_duplicates(inplace=True)
df.rename(columns={'addr:city':'city'}, inplace=True)
df.drop(columns=['addr:postcode','@id'], inplace=True)

#3)Correcting inconsistent formats
#creating category map of the shop category in the dataframe to group similar shops in one category
category_map = {
    # Food & Grocery
    'supermarket': 'Food & Grocery',
    'convenience': 'Food & Grocery',
    'bakery': 'Food & Grocery',
    'butcher': 'Food & Grocery',
    'seafood': 'Food & Grocery',
    'greengrocer': 'Food & Grocery',
    'alcohol': 'Food & Grocery',
    'wine': 'Food & Grocery',
    'beverages': 'Food & Grocery',
    'tea': 'Food & Grocery',
    'coffee': 'Food & Grocery',
    'water': 'Food & Grocery',
    'confectionery': 'Food & Grocery',
    'chocolate': 'Food & Grocery',
    'pastry': 'Food & Grocery',
    'farsaan': 'Food & Grocery',
    'nuts': 'Food & Grocery',
    'spices': 'Food & Grocery',
    'sweet, dry fruit and farsan': 'Food & Grocery',
    'frozen_food': 'Food & Grocery',
    'dairy': 'Food & Grocery',
    'milk_shop': 'Food & Grocery',
    'ice_cream': 'Food & Grocery',
    'food': 'Food & Grocery',
    'grocery': 'Food & Grocery',
    'fast_food': 'Food & Grocery', # Grouping with F&G for this model

    # Fashion & Apparel
    'clothes': 'Fashion',
    'shoes': 'Fashion',
    'boutique': 'Fashion',
    'fashion_accessories': 'Fashion',
    'jewelry': 'Fashion',
    'watches': 'Fashion',
    'leather': 'Fashion',
    'tailor': 'Fashion',
    'fabric': 'Fashion',
    'clothes;handwoven;ikat': 'Fashion',
    'shoes;clothes': 'Fashion',

    # Health & Beauty
    'hairdresser': 'Health & Beauty',
    'beauty': 'Health & Beauty',
    'cosmetics': 'Health & Beauty',
    'perfumery': 'Health & Beauty',
    'chemist': 'Health & Beauty',
    'pharmacy': 'Health & Beauty',
    'optician': 'Health & Beauty',
    'tattoo': 'Health & Beauty',
    'piercing': 'Health & Beauty',
    'massage': 'Health & Beauty',
    'health_food': 'Health & Beauty',
    'nutrition_supplements': 'Health & Beauty',
    'herbalist': 'Health & Beauty',
    'hearing_aids': 'Health & Beauty',
    'medical_supply': 'Health & Beauty',
    'diapers': 'Health & Beauty',
    'hairdresser;tattoo': 'Health & Beauty',

    # Home & Garden
    'doityourself': 'Home & Garden',
    'hardware': 'Home & Garden',
    'furniture': 'Home & Garden',
    'bed': 'Home & Garden',
    'interior_decoration': 'Home & Garden',
    'flooring': 'Home & Garden',
    'tiles': 'Home & Garden',
    'paint': 'Home & Garden',
    'houseware': 'Home & Garden',
    'household_linen': 'Home & Garden',
    'lighting': 'Home & Garden',
    'bathroom_furnishing': 'Home & Garden',
    'garden_centre': 'Home & Garden',
    'florist': 'Home & Garden',
    'appliance': 'Home & Garden',
    'hardware; household; electrical; accessories': 'Home & Garden',

    # Electronics & Auto
    'electronics': 'Electronics & Auto',
    'mobile_phone': 'Electronics & Auto',
    'computer': 'Electronics & Auto',
    'radiotechnics': 'Electronics & Auto',
    'hifi': 'Electronics & Auto',
    'telecommunication': 'Electronics & Auto',
    'car': 'Electronics & Auto',
    'motorcycle': 'Electronics & Auto',
    'car_parts': 'Electronics & Auto',
    'motorcycle_parts': 'Electronics &S Auto',
    'tyres': 'Electronics & Auto',
    'battery': 'Electronics & Auto',
    'gas': 'Electronics & Auto',
    'fuel': 'Electronics & Auto',

    # General Retail & Department Stores
    'mall': 'General Retail',
    'department_store': 'General Retail',
    'variety_store': 'General Retail',
    'kiosk': 'General Retail',
    'wholesale': 'General Retail',
    'wholesale market': 'General Retail',
    'market_place': 'General Retail',
    'general': 'General Retail',
    'second_hand': 'General Retail',
    'duty_free': 'General Retail',
    'country_store': 'General Retail',
    'clothes, food, textiles, beauty, jewellery, accessories, household, decor': 'General Retail',
    'clothing, accessory, household, decor, gift': 'General Retail',

    # Specialty & Hobby
    'bicycle': 'Specialty & Hobby',
    'books': 'Specialty & Hobby',
    'stationery': 'Specialty & Hobby',
    'gift': 'Specialty & Hobby',
    'toys': 'Specialty & Hobby',
    'craft': 'Specialty & Hobby',
    'art': 'Specialty & Hobby',
    'model': 'Specialty & Hobby',
    'musical_instrument': 'Specialty & Hobby',
    'music': 'Specialty & Hobby',
    'video_games': 'Specialty & Hobby',
    'pet': 'Specialty & Hobby',
    'fishing': 'Specialty & Hobby',
    'sports': 'Specialty & Hobby',
    'party': 'Specialty & Hobby',
    'frame': 'Specialty & Hobby',
    'religion': 'Specialty & Hobby',
    'pyrotechnics': 'Specialty & Hobby',
    'baby_goods': 'Specialty & Hobby',

    # Services
    'travel_agency': 'Services',
    'car_repair': 'Services',
    'motorcycle_repair': 'Services',
    'laundry': 'Services',
    'dry_cleaning': 'Services',
    'ticket': 'Services',
    'copyshop': 'Services',
    'photo': 'Services',
    'photo_studio': 'Services',
    'bookmaker': 'Services',
    'pet_grooming': 'Services',
    'repair': 'Services',
    'shoe_repair': 'Services',
    'printing': 'Services',
    'electrical': 'Services',
    'funeral_directors': 'Services',
    'sewing': 'Services',
    'gold_buyer': 'Services',
    'mobile_repair': 'Services',
    'cable_tv': 'Services',

    # Other / Unclear
    'yes': 'Other',  # This is clearly bad data
    'trade': 'Other',
    'outpost': 'Other',
    'beau': 'Other', # Incomplete word
    'clothes;art;handicraft': 'Other', # Too mixed
    'gift;art;clothing;book;magazine;decor;accessory;household;local': 'General Retail' # This is a general store
}

#here we perform data cleaning and pre-processing of the latitude and logitude  column ,where the values are null we fill this values from first trying to get information of latiude and longitude using pincode and country , but when we have NA value in pinocde column as well we try to fill latitude , longitude using city center latitude , longitude for the letout values .
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut
import time
import os
import json
import numpy as np



from google.colab import drive
drive.mount('/content/drive')

DRIVE_PATH = '/content/drive/MyDrive/Colab_Data/'
CACHE_FILE = os.path.join(DRIVE_PATH, 'pincode_cache.json')
FINAL_FILE = os.path.join(DRIVE_PATH, 'FINAL_DATASET_WITH_COORDS.csv')
os.makedirs(DRIVE_PATH, exist_ok=True)

#Here we load existing cache files if it is present
if os.path.exists(CACHE_FILE):
    print(f"Loading existing pincode cache from {CACHE_FILE}")
    with open(CACHE_FILE, 'r') as f:

        pincode_location_map_str_keys = json.load(f)

        pincode_location_map = {float(k) if k != 'None' else np.nan: v for k, v in pincode_location_map_str_keys.items()}
else:
    print("No cache file found. Starting a new one.")
    pincode_location_map = {}

#WE first find all unique pincode  where the latitude and longitude is null
missing_mask = df['latitude'].isnull() | df['longitude'].isnull()
pincodes_to_check = df[missing_mask]['pincode'].unique()


pincodes_to_find = [p for p in pincodes_to_check if p not in pincode_location_map]
print(f"Found {len(pincodes_to_find)} new unique pincodes to search for.")


if len(pincodes_to_find) > 0:
    geolocator = Nominatim(user_agent="my-location-finder-app-v6")

    for i, pincode in enumerate(pincodes_to_find):
       #Here we handle the cases when pinoce is NAN
        if pd.isna(pincode):
            pincode_location_map[np.nan] = (None, None)
            continue

        query = str(pincode).split('.')[0] + ", India"
        print(f"Querying {i+1}/{len(pincodes_to_find)}: {query}")

        try:
            time.sleep(1.5)
            location = geolocator.geocode(query, timeout=10)

            if location:
                pincode_location_map[pincode] = (location.latitude, location.longitude)
            else:
                pincode_location_map[pincode] = (None, None)

        except (GeocoderTimedOut, Exception) as e:
            print(f"  > ERROR for {query}: {e}. Skipping.")
            pincode_location_map[pincode] = (None, None)


        if (i + 1) % 10 == 0:
            print(f"\n*** CHECKPOINT: Saving cache to {CACHE_FILE} ***\n")

            pincode_location_map_str_keys = {str(k): v for k, v in pincode_location_map.items()}
            with open(CACHE_FILE, 'w') as f:
                json.dump(pincode_location_map_str_keys, f)



    print("API lookup complete. Saving final cache.")
    pincode_location_map_str_keys = {str(k): v for k, v in pincode_location_map.items()}
    with open(CACHE_FILE, 'w') as f:
        json.dump(pincode_location_map_str_keys, f)
else:
    print("All necessary pincodes are already in the cache.")

# Apply the map to your DataFrame (this is instant)
print("Applying coordinates to DataFrame...")

#Here it is function to map coords
def map_coords(row, coord_type):
    if pd.isna(row[coord_type]):
        coords = pincode_location_map.get(row['pincode'])
        if coord_type == 'latitude':
            return coords[0] if coords else None
        if coord_type == 'longitude':
            return coords[1] if coords else None
    return row[coord_type]

df['latitude'] = df.apply(map_coords, axis=1, coord_type='latitude')
df['longitude'] = df.apply(map_coords, axis=1, coord_type='longitude')


# Here we Imute the reminaing NAN with city center
print("Imputing remaining missing locations by city...")

# Calculate the average lat/lon for each city *from the rows that have data*
city_centroids_lat = df.groupby('city')['latitude'].mean().to_dict()
city_centroids_lon = df.groupby('city')['longitude'].mean().to_dict()

still_missing_mask = df['latitude'].isnull()
print(f"Found {still_missing_mask.sum()} rows to impute with city-average location.")


df.loc[still_missing_mask, 'latitude'] = df[still_missing_mask]['city'].map(city_centroids_lat)
df.loc[still_missing_mask, 'longitude'] = df[still_missing_mask]['city'].map(city_centroids_lon)


original_rows = len(df)
df.dropna(subset=['latitude', 'longitude'], inplace=True)
rows_dropped = original_rows - len(df)

print(f"Imputation complete. Dropped {rows_dropped} rows (that had no pincode AND no city).")
print(f"Process complete. You have {len(df)} rows.")

df.to_csv(FINAL_FILE, index=False)
print(f"Final CSV with filled coordinates saved to: {FINAL_FILE}")

#Here we find the missing values in pincodes using the pincode.csv dataset using latitude and longitude of the dataset , it there are no enteries in the pincode.csv dataset then we use a API ,Nominatim to fetch the pincode from the API.


#loading and checking the data

try:
  if 'Unnamed: 0' in df.columns:
        df = df.drop(columns=['Unnamed: 0'])
        print(f"Successfully loaded initial DataFrame. Shape: {df.shape}")
except NameError:
    print("Error: DataFrame 'df' not found. Please load your 'final_dataset1.csv' into a DataFrame named 'df' first.")
    raise

try:
    drive.mount('/content/drive')
    DRIVE_PATH = '/content/drive/MyDrive/Colab_Data/'

    BACKUP_FILE = os.path.join(DRIVE_PATH, 'PINCODE_backup.csv')
    FINAL_FILE = os.path.join(DRIVE_PATH, 'FINAL_DATASET_WITH_PINCODES.csv')
    os.makedirs(DRIVE_PATH, exist_ok=True)
    print(f"Google Drive mounted. Backup file will be at: {BACKUP_FILE}")
except Exception as e:
    print(f"Error mounting Google Drive: {e}")
    raise


if os.path.exists(BACKUP_FILE):
    print(f"Loading previous progress from {BACKUP_FILE}...")
    df = pd.read_csv(BACKUP_FILE)
else:
    print(f"No pincode backup file found. Starting from the original 'df'.")

    df['pincode'] = df['pincode'].astype(str).replace('nan', np.nan)


# Here we find Rows that STILL need a Pincode
missing_mask = df['pincode'].isnull()
rows_to_fill_indices = df[missing_mask].index
total_rows_to_fill = len(rows_to_fill_indices)

print(f"Found {total_rows_to_fill} rows that need a pincode.")

# Loop, Geocode, and Save with Checkpoints
if total_rows_to_fill > 0:
    geolocator = Nominatim(user_agent="my-pincode-finder-app-v1")

    for i, index in enumerate(rows_to_fill_indices):
        row = df.loc[index]

        #Here we craete the coordinate string for the reverse lookup
        query = f"{row['latitude']}, {row['longitude']}"

        print(f"Processing {i+1}/{total_rows_to_fill}: Querying coordinates {query}...")

        try:
            time.sleep(1.5)
            location = geolocator.reverse(query, language='en', addressdetails=True, timeout=10)

            if location and 'address' in location.raw and 'postcode' in location.raw['address']:
                found_pincode = location.raw['address']['postcode']
                print(f"  > Success: Found pincode {found_pincode}")
                df.loc[index, 'pincode'] = found_pincode
            else:
                print("  > FAILED: Could not find a pincode for these coordinates.")

                df.loc[index, 'pincode'] = 'Not_Found'

        except (GeocoderTimedOut, Exception) as e:
            print(f"  > ERROR: {e}. Will retry on next run.")
            pass


        if (i + 1) % 25 == 0:
            print(f"\n*** CHECKPOINT: Saving progress to {BACKUP_FILE} ***\n")
            df.to_csv(BACKUP_FILE, index=False)
else:
    print("No missing pincodes to find.")

print("Reverse geocoding complete! Saving final file.")
df.to_csv(FINAL_FILE, index=False)

print(f"\nProcess complete. All data saved to {FINAL_FILE}")

# Ensure the 'shop' column is lowercase and stripped of whitespace
df['shop'] = df['shop'].str.lower().str.strip()
# Create a new column 'shop_category' using the map
df['shop_category'] = df['shop'].map(category_map)
# Any shop type not in our map (including nan) will be set to 'Other'
df['shop_category'] = df['shop_category'].fillna('Other')
# Now you can check the results:
print("--- New Category Counts ---")
print(df['shop_category'].value_counts())
df.drop(columns='shop')
df.rename(columns={'addr:city':'city'},inplace=True)
df['latitude'].isna().sum()

#4)Correcting inconsistent formats.
#Here we are reducing the number of columns of demographics data by combining them that make meaning full attribute.
#So starting from scheduled cast and schuled tribes these atributes are merged to form SC_ST_RATE similarly we do with other attributes as well

import numpy as np
df['pc11_pca_clean_shrid_pc11_pca_p_sc']=df['pc11_pca_clean_shrid_pc11_pca_p_sc'].fillna(0)
df['pc11_pca_clean_shrid_pc11_pca_p_st']=df['pc11_pca_clean_shrid_pc11_pca_p_st'].fillna(0)
df['TOT_P']=df['TOT_P'].fillna(0)
total_sc_st=df['pc11_pca_clean_shrid_pc11_pca_p_sc']+df['pc11_pca_clean_shrid_pc11_pca_p_st']
df['SC_ST_RATE']=np.where(df['TOT_P']>0 ,total_sc_st/df['TOT_P'],0).round(4)
#In feature enigineering repalced all this columns with one column of SC_ST_Rate of population in area
df.drop(columns=['pc11_pca_clean_shrid_pc11_pca_m_sc','pc11_pca_clean_shrid_pc11_pca_f_sc','pc11_pca_clean_shrid_pc11_pca_p_st','pc11_pca_clean_shrid_pc11_pca_m_st','pc11_pca_clean_shrid_pc11_pca_f_st'], inplace=True)
#feature engineriing on litracy rate
df['LIT_P']=df['LIT_P'].fillna(0)
df['TOT_P']=df['TOT_P'].fillna(0)
Total_literacy_rate=np.where(df['TOT_P']>0,df['LIT_P']/df['TOT_P'],0).round(4)
df['Total_literacy_rate']=Total_literacy_rate
df['pc11_pca_clean_shrid_pc11_pca_p_ill']=df['pc11_pca_clean_shrid_pc11_pca_p_ill'].fillna(0)
df['TOT_P']=df['TOT_P'].fillna(0)
df['ill_rate']=np.where(df['TOT_P']>0,df['pc11_pca_clean_shrid_pc11_pca_p_ill']/df['TOT_P'],0)
df.drop(columns=['pc11_pca_clean_shrid_pc11_pca_p_ill','pc11_pca_clean_shrid_pc11_pca_m_ill','pc11_pca_clean_shrid_pc11_pca_f_ill',])
df['pc11_pca_clean_shrid_pc11_pca_p_06']=df['pc11_pca_clean_shrid_pc11_pca_p_06'].fillna(0)
df['TOT_P']=df['TOT_P'].fillna(0)
df['childreen_population_rate']=np.where(df['TOT_P']>0,df['pc11_pca_clean_shrid_pc11_pca_p_06']/df['TOT_P'],0)
df.drop(columns=['pc11_pca_clean_shrid_pc11_pca_m_06','pc11_pca_clean_shrid_pc11_pca_f_06','pc11_pca_clean_shrid_pc11_pca_p_06'],inplace=True)
#Mainworker atrribute reprsents the worker worked for aleast 6 months or more in our dataset we have 4 types of main workers that are cultivators, household industry , agricultural labourers, and other workers this the population going to offices , bussiness owners also major contributor for economic growth
df['pc11_pca_clean_shrid_pc11_pca_main_cl_p']=df['pc11_pca_clean_shrid_pc11_pca_main_cl_p'].fillna(0)
df['TOT_P']=df['TOT_P'].fillna(0)
df['Cultivators_pulation_rate']=np.where(df['TOT_P']>0,df['pc11_pca_clean_shrid_pc11_pca_main_cl_p']/df['TOT_P'],0)
df.drop(columns=['pc11_pca_clean_shrid_pc11_pca_main_cl_m','pc11_pca_clean_shrid_pc11_pca_main_cl_f','pc11_pca_clean_shrid_pc11_pca_main_cl_p'], inplace=True)
df['pc11_pca_clean_shrid_pc11_pca_main_al_p']=df['pc11_pca_clean_shrid_pc11_pca_main_al_p'].fillna(0)
df['TOT_P']=df['TOT_P'].fillna(0)
df['agricultural_population_rate']=np.where(df['TOT_P']>0,df['pc11_pca_clean_shrid_pc11_pca_main_al_p']/df['TOT_P'],0)
df.drop(columns=['pc11_pca_clean_shrid_pc11_pca_main_al_m','pc11_pca_clean_shrid_pc11_pca_main_al_f','pc11_pca_clean_shrid_pc11_pca_main_al_p'], inplace=True)
df['pc11_pca_clean_shrid_pc11_pca_main_hh_p']=df['pc11_pca_clean_shrid_pc11_pca_main_hh_p'].fillna(0)
df['TOT_P'] = df['TOT_P'].fillna(0)
df['household_industry_population_rate'] = np.where(df['TOT_P'] > 0, df['pc11_pca_clean_shrid_pc11_pca_main_hh_p'] / df['TOT_P'], 0)
df['OTHER_W']=df['OTHER_W'].fillna(0)
df['Other_Worker_population_rate']=np.where(df['TOT_P']>0,df['OTHER_W']/df['TOT_P'],0)
df.drop(columns=['OTHER_W','pc11_pca_clean_shrid_pc11_pca_main_ot_m','pc11_pca_clean_shrid_pc11_pca_main_ot_f',])
df['pc11_pca_clean_shrid_pc11_pca_margwork_p']=df['pc11_pca_clean_shrid_pc11_pca_margwork_p'].fillna(0)
df['marginial_workers_population_rate']=np.where(df['TOT_P']>0,df['pc11_pca_clean_shrid_pc11_pca_margwork_p']/df['TOT_P'],0)
df.drop(columns=['pc11_pca_clean_shrid_pc11_pca_margwork_p','pc11_pca_clean_shrid_pc11_pca_margwork_m','pc11_pca_clean_shrid_pc11_pca_margwork_f','pc11_pca_clean_shrid_pc11_pca_marg_cl_p','pc11_pca_clean_shrid_pc11_pca_marg_cl_m','pc11_pca_clean_shrid_pc11_pca_marg_cl_f','pc11_pca_clean_shrid_pc11_pca_marg_al_p','pc11_pca_clean_shrid_pc11_pca_marg_al_m','pc11_pca_clean_shrid_pc11_pca_marg_al_f','pc11_pca_clean_shrid_pc11_pca_marg_hh_p','pc11_pca_clean_shrid_pc11_pca_marg_hh_m','pc11_pca_clean_shrid_pc11_pca_marg_hh_f','pc11_pca_clean_shrid_pc11_pca_marg_ot_p','pc11_pca_clean_shrid_pc11_pca_marg_ot_m','pc11_pca_clean_shrid_pc11_pca_marg_ot_f',
],inplace=True)

df.drop(columns=['LIT_P','pc11_pca_clean_shrid_pc11_pca_m_lit','pc11_pca_clean_shrid_pc11_pca_f_lit','pc11_pca_clean_shrid_pc11_pca_p_ill','pc11_pca_clean_shrid_pc11_pca_m_ill','pc11_pca_clean_shrid_pc11_pca_f_ill'],inplace=True)
df['pc11_pca_clean_shrid_pc11_pca_margwork36_p']=df['pc11_pca_clean_shrid_pc11_pca_margwork36_p'].fillna(0)
df['pc11_pca_clean_shrid_pc11_pca_margwork03_p']=df['pc11_pca_clean_shrid_pc11_pca_margwork03_p'].fillna(0)
marginal_worker=df['pc11_pca_clean_shrid_pc11_pca_margwork36_p']+df['pc11_pca_clean_shrid_pc11_pca_margwork03_p']
df['marginial_workers_lessthan6months_population_rate']=np.where(df['TOT_P']>0,marginal_worker/df['TOT_P'],0)
df.drop(columns=['pc11_pca_clean_shrid_pc11_pca_margwork36_p','pc11_pca_clean_shrid_pc11_pca_margwork36_m','pc11_pca_clean_shrid_pc11_pca_margwork36_f','pc11_pca_clean_shrid_pc11_pca_marg_cl36_p','pc11_pca_clean_shrid_pc11_pca_marg_cl36_m','pc11_pca_clean_shrid_pc11_pca_marg_cl36_f','pc11_pca_clean_shrid_pc11_pca_marg_al36_p','pc11_pca_clean_shrid_pc11_pca_marg_al36_m','pc11_pca_clean_shrid_pc11_pca_marg_al36_f','pc11_pca_clean_shrid_pc11_pca_marg_hh36_p','pc11_pca_clean_shrid_pc11_pca_marg_hh36_m','pc11_pca_clean_shrid_pc11_pca_marg_hh36_f','pc11_pca_clean_shrid_pc11_pca_marg_ot36_p','pc11_pca_clean_shrid_pc11_pca_marg_ot36_m','pc11_pca_clean_shrid_pc11_pca_marg_ot36_f','pc11_pca_clean_shrid_pc11_pca_margwork03_p','pc11_pca_clean_shrid_pc11_pca_margwork03_m','pc11_pca_clean_shrid_pc11_pca_margwork03_f','pc11_pca_clean_shrid_pc11_pca_marg_cl03_p','pc11_pca_clean_shrid_pc11_pca_marg_cl03_m','pc11_pca_clean_shrid_pc11_pca_marg_cl03_f','pc11_pca_clean_shrid_pc11_pca_marg_al03_p','pc11_pca_clean_shrid_pc11_pca_marg_al03_m','pc11_pca_clean_shrid_pc11_pca_marg_al03_f','pc11_pca_clean_shrid_pc11_pca_marg_hh03_p','pc11_pca_clean_shrid_pc11_pca_marg_hh03_m','pc11_pca_clean_shrid_pc11_pca_marg_hh03_f','pc11_pca_clean_shrid_pc11_pca_marg_ot03_p','pc11_pca_clean_shrid_pc11_pca_marg_ot03_m','pc11_pca_clean_shrid_pc11_pca_marg_ot03_f','pc11_pca_clean_shrid_pc11_pca_non_work_p','pc11_pca_clean_shrid_pc11_pca_non_work_m','pc11_pca_clean_shrid_pc11_pca_non_work_f'],inplace=True)
df['TOT_W']=df['TOT_W'].fillna(0)
df['total_working_population_rate']=np.where(df['TOT_P']>0,df['TOT_W']/df['TOT_P'],0)
df.drop(columns=['TOT_W','pc11_pca_clean_shrid_pc11_pca_tot_work_m','pc11_pca_clean_shrid_pc11_pca_tot_work_f','pc11_pca_clean_shrid_pc11_pca_mainwork_p','pc11_pca_clean_shrid_pc11_pca_mainwork_m','pc11_pca_clean_shrid_pc11_pca_mainwork_f','pc11_pca_clean_shrid_pc11_pca_main_hh_p','pc11_pca_clean_shrid_pc11_pca_main_hh_m','pc11_pca_clean_shrid_pc11_pca_main_hh_f','OTHER_W','pc11_pca_clean_shrid_pc11_pca_main_ot_m','pc11_pca_clean_shrid_pc11_pca_main_ot_f'],inplace=True)
#Mainworker atrribute reprsents the worker worked for aleast 6 months or more in our dataset we have 4 types of main workers that are cultivators, household industry , agricultural labourers, and other workers this the population going to offices , bussiness owners also major contributor for economic growth
df['Street_Address']=df['name:en'].str.cat([df['addr:street']],sep=",",na_rep=" ")
df.drop(columns=['addr:street','name:en','pc11_pca_clean_shrid_pc11_pca_no_hh','pc11_pca_clean_shrid_pc11_pca_tot_m','pc11_pca_clean_shrid_pc11_pca_tot_f','pc11_pca_clean_shrid_pc11_pca_p_sc'])
df.drop(columns=['fid_2','shrid2','pc11_id','n','geometry_t','polysource','pc11_pca_clean_shrid_pc11_pca_no_hh','pc11_pca_clean_shrid_pc11_pca_tot_m','pc11_pca_clean_shrid_pc11_pca_tot_f','pc11_pca_clean_shrid_pc11_pca_p_sc'],inplace=True)
df.drop(columns=['addr:street','name:en','level'], inplace=True)

df.columns

from google.colab import drive
drive.mount('/content/drive')

df.head()

#Saving the feature engineered dataset
df.to_csv('dataframe1.csv',index=False)

# Feature engineering
#1)here we perfrom Engineering Geospatial Features 2)Then we perfrom Engineering Demographic Features (PCA) 3)Then we combine engineering features 4)Then we perfrom Dimensionality reduction & feature extraction
import pandas as pd
import geopandas as gpd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import BallTree # For efficient density calculation
import warnings


warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)



print("--- Initial Data Loaded ---")
print(f"Original shape: {df.shape}")

df.dropna(subset=['latitude', 'longitude'], inplace=True)
print(f"Shape after dropping rows with missing coordinates: {df.shape}")

# Here we Convert dataframe to GeoDataFrame
gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df.longitude, df.latitude),
    crs="EPSG:4326"  # Standard WGS84 Lat/Lon
)
print("Converted DataFrame to GeoDataFrame.")


print("\n--- Engineering Geospatial Features ---")

target_crs = "EPSG:32643" # UTM Zone 43N, suitable for Mumbai (units in meters)
print(f"Projecting coordinates to {target_crs}...")
gdf_proj = gdf.to_crs(target_crs)

#Here we define the co-ordinates of all 5 hubs
hub_data = {
    'hub_name': ['CST', 'NaviMumbai', 'Thane', 'Virar', 'Nalasopara'],
    'lon': [72.8351, 72.9986, 72.9781, 72.8095, 72.8159],
    'lat': [18.9401, 19.0760, 19.2183, 19.4582, 19.4184]
}
df_hubs = pd.DataFrame(hub_data)


gdf_hubs = gpd.GeoDataFrame(
    df_hubs,
    geometry=gpd.points_from_xy(df_hubs.lon, df_hubs.lat),
    crs="EPSG:4326"
)


gdf_hubs_proj = gdf_hubs.to_crs(target_crs)
print(f"Created and projected {len(gdf_hubs_proj)} hubs.")

# Here we calculate distances from each shop to ALL hubs
# This creates a new DataFrame: rows = shops, columns = hubs, values = distance in meters
print("Calculating distances from each shop to all hubs...")
all_distances = gdf_proj.geometry.apply(lambda shop_geom: gdf_hubs_proj.distance(shop_geom))
all_distances.columns = gdf_hubs_proj['hub_name']


gdf['distance_to_nearest_hub_km'] = all_distances.min(axis=1) / 1000

# Find the *name* of the hub with the minimum distance
gdf['nearest_hub'] = all_distances.idxmin(axis=1)

print("Created 'distance_to_nearest_hub_km' and 'nearest_hub' features.")

coords = np.vstack([gdf_proj.geometry.x, gdf_proj.geometry.y]).T


tree = BallTree(coords)


radii = {'500m': 500, '1000m': 1000}
for name, radius_m in radii.items():
    counts = tree.query_radius(coords, r=radius_m, count_only=True)
    gdf[f'shops_within_{name}'] = counts - 1
    print(f"Calculated shop density within {name}.")

print("\n--- Engineering Demographic Features (PCA) ---")

demographic_rates = [
    'SC_ST_RATE', 'Total_literacy_rate', 'childreen_population_rate',
    'Cultivators_pulation_rate', 'agricultural_population_rate',
    'household_industry_population_rate', 'Other_Worker_population_rate',
    'marginial_workers_population_rate'
]
df_demographics = gdf[demographic_rates].copy()
df_demographics.fillna(df_demographics.median(), inplace=True) # Impute with median

pca_scaler = StandardScaler()
demographics_scaled = pca_scaler.fit_transform(df_demographics)

pca = PCA(n_components=0.90)
demographic_pca_components = pca.fit_transform(demographics_scaled)
print(f"PCA: Reduced {len(demographic_rates)} features to {pca.n_components_} components.")

pc_columns = [f'Demographic_PC_{i+1}' for i in range(pca.n_components_)]
df_pca = pd.DataFrame(demographic_pca_components, columns=pc_columns, index=df_demographics.index)


print("\n--- Keeping 'shop_category' and new 'nearest_hub' for later encoding ---")


# Here we perform derived Features (Target Transformation) ---
print("\n--- Transforming Target Variable (Revenue) ---")
gdf['log_revenue'] = np.log1p(gdf['revenue'])
print("Created 'log_revenue'.")


print("\n--- Combining Engineered Features ---")


original_cols_to_keep = ['shop_category', 'nearest_hub', 'revenue', 'log_revenue', 'city', 'pincode']


geo_features = ['latitude', 'longitude']



df_engineered = pd.concat([
    gdf[original_cols_to_keep + geo_features], # Keep lat/lon for reference
    gdf[['distance_to_nearest_hub_km', 'shops_within_500m', 'shops_within_1000m']],
    df_pca
], axis=1)


rows_before_final_drop = len(df_engineered)
df_engineered.dropna(inplace=True)
rows_after_final_drop = len(df_engineered)
print(f"Final check for NaNs dropped {rows_before_final_drop - rows_after_final_drop} rows.")
print(f"Final engineered dataset shape: {df_engineered.shape}")


output_file = 'final_dataset_engineered_multihub.csv'
df_engineered.to_csv(output_file, index=False)

print(f"\nâœ… Successfully saved feature-engineered data to '{output_file}'")
print("\n--- Engineered Data Head ---")
print(df_engineered.head())

df_engineered

df_engineered['shop_category'].unique()

#Here we perfrom Exploratory data analysis
#1)First we perform univariate analysis 2)Thenn we perform bivariate Analysis (Finding Relationships) 3)Then we perform Geospatial Analysis (Putting it on a Map)
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

print("--- EDA Started ---")


warnings.filterwarnings('ignore')
sns.set(style="whitegrid")


file_path = 'final_dataset_engineered_multihub.csv'
try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found. Please re-run the feature engineering script first.")
    raise

print(f"Loaded dataset 'final_dataset_engineered_multihub.csv' with shape: {df.shape}")

#1)First we perform univariate analysis
print("\nGenerating Univariate Plots...")

# a)Target Variable (Revenue)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(df['revenue'], bins=50, kde=True)
plt.title('Original Revenue Distribution-skewed')
plt.xlabel('Revenue')

plt.subplot(1, 2, 2)
sns.histplot(df['log_revenue'], bins=50, kde=True, color='green')
plt.title('Log-Transformed Revenue Distribution-Normalized')
plt.xlabel('Log(Revenue + 1)')
plt.tight_layout()
plt.savefig('eda_revenue_distribution.png')
print("Saved eda_revenue_distribution.png")

# b)Categorical Features (Shop Category & Nearest Hub)
plt.figure(figsize=(14, 7))
plt.subplot(1, 2, 1)
sns.countplot(y='shop_category', data=df, order=df['shop_category'].value_counts().index, palette='Blues_r')
plt.title('Shop Category Counts')
plt.xlabel('Count')
plt.ylabel('Shop Category')

plt.subplot(1, 2, 2)
sns.countplot(x='nearest_hub', data=df, order=df['nearest_hub'].value_counts().index, palette='Greens_r')
plt.title('Nearest Hub Counts')
plt.xlabel('Nearest Hub')
plt.ylabel('Count')
plt.tight_layout()
plt.savefig('eda_categorical_counts.png')
print("Saved eda_categorical_counts.png")

# c)Engineered Numerical Features (Distance & Density)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(df['distance_to_nearest_hub_km'], bins=30, kde=True, color='purple')
plt.title('Distance to Nearest Hub')
plt.xlabel('Distance (km)')

plt.subplot(1, 2, 2)
sns.histplot(df['shops_within_1000m'], bins=30, kde=True, color='orange')
plt.title('Shop Density (within 1000m)')
plt.xlabel('Shops within 1000m')
plt.tight_layout()
plt.savefig('eda_geo_features_distribution.png')
print("Saved eda_geo_features_distribution.png")

# d)Demographic PCA Components
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(df['Demographic_PC_1'], bins=30, kde=True)
plt.title('Demographic Principal Component 1')
plt.xlabel('PC 1 Value')

plt.subplot(1, 2, 2)
sns.histplot(df['Demographic_PC_2'], bins=30, kde=True)
plt.title('Demographic Principal Component 2')
plt.xlabel('PC 2 Value')
plt.tight_layout()
plt.savefig('eda_pca_distribution.png')
print("Saved eda_pca_distribution.png")

#Bivariate Analysis (Finding Relationships)
print("\nGenerating Bivariate Plots...")

# a) Categorical vs. Revenue (Box Plots)
plt.figure(figsize=(14, 7))
plt.subplot(1, 2, 1)
cat_order = df.groupby('shop_category')['log_revenue'].median().sort_values(ascending=False).index
sns.boxplot(y='shop_category', x='log_revenue', data=df, order=cat_order, palette='Spectral')
plt.title('Log-Revenue by Shop Category')
plt.xlabel('Log(Revenue + 1)')
plt.ylabel('Shop Category')

plt.subplot(1, 2, 2)
hub_order = df.groupby('nearest_hub')['log_revenue'].median().sort_values(ascending=False).index
sns.boxplot(x='nearest_hub', y='log_revenue', data=df, order=hub_order, palette='Pastel1')
plt.title('Log-Revenue by Nearest Hub')
plt.xlabel('Nearest Hub')
plt.ylabel('Log(Revenue + 1)')
plt.tight_layout()
plt.savefig('eda_categorical_vs_revenue.png')
print("Saved eda_categorical_vs_revenue.png")

# b) Numerical vs. Revenue (Scatter Plots)
plt.figure(figsize=(15, 10))
plt.subplot(2, 2, 1)
sns.scatterplot(x='distance_to_nearest_hub_km', y='log_revenue', data=df, alpha=0.5)
plt.title('Revenue vs. Distance to Hub')
plt.xlabel('Distance (km)')
plt.ylabel('Log(Revenue + 1)')

plt.subplot(2, 2, 2)
sns.scatterplot(x='shops_within_1000m', y='log_revenue', data=df, alpha=0.5)
plt.title('Revenue vs. Local Shop Density')
plt.xlabel('Shops within 1000m')
plt.ylabel('Log(Revenue + 1)')

plt.subplot(2, 2, 3)
sns.scatterplot(x='Demographic_PC_1', y='log_revenue', data=df, alpha=0.5)
plt.title('Revenue vs. Demographic PC 1')
plt.xlabel('Demographic PC 1')
plt.ylabel('Log(Revenue + 1)')

plt.subplot(2, 2, 4)
sns.scatterplot(x='Demographic_PC_2', y='log_revenue', data=df, alpha=0.5)
plt.title('Revenue vs. Demographic PC 2')
plt.xlabel('Demographic PC 2')
plt.ylabel('Log(Revenue + 1)')
plt.tight_layout()
plt.savefig('eda_scatter_vs_revenue.png')
print("Saved eda_scatter_vs_revenue.png")

# c) Correlation Heatmap
plt.figure(figsize=(12, 8))
corr_cols = ['log_revenue', 'distance_to_nearest_hub_km', 'shops_within_500m', 'shops_within_1000m', 'Demographic_PC_1', 'Demographic_PC_2']
corr_matrix = df[corr_cols].corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of Engineered Numerical Features')
plt.tight_layout()
plt.savefig('eda_correlation_heatmap.png')
print("Saved eda_correlation_heatmap.png")

# Geospatial Analysis (Putting it on a Map)
print("\nGenerating Geospatial Map Plots...")

# Convert the DataFrame to a GeoDataFrame for map plotting
gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df.longitude, df.latitude),
    crs="EPSG:4326"  # Standard WGS84 Lat/Lon
)

# a) Map of Revenue
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
gdf.plot(ax=ax,
         column='log_revenue', # Color-code by log_revenue
         legend=True,
         markersize=5,
         cmap='viridis',
         legend_kwds={'label': "Log(Revenue + 1)", 'orientation': "horizontal"})
plt.title('Geospatial Distribution of Revenue')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.savefig('eda_map_revenue.png')
print("Saved eda_map_revenue.png")

# b) Map of Shop Categories
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
gdf.plot(ax=ax,
         column='shop_category',
         legend=True,
         markersize=5,
         cmap='tab20',
         legend_kwds={'title': "Shop Category", 'bbox_to_anchor': (1.05, 1), 'loc': 'upper left'})
plt.title('Geospatial Distribution of Shop Categories')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.tight_layout()
plt.savefig('eda_map_shop_categories.png')
print("Saved eda_map_shop_categories.png")

# c) Map of Nearest Hubs
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
gdf.plot(ax=ax,
         column='nearest_hub',
         legend=True,
         markersize=5,
         cmap='Set1',
         legend_kwds={'title': "Nearest Hub", 'bbox_to_anchor': (1.05, 1), 'loc': 'upper left'})
plt.title('Geospatial Distribution by Nearest Hub')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.tight_layout()
plt.savefig('eda_map_nearest_hubs.png')
print("Saved eda_map_nearest_hubs.png")

print("\nEDA Complete. All Plots saved ")

# Outlier Analysis
#USing Box plots
#We Select numeric columns
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Numeric features:")
print(numeric_cols)
# WE Set the figure size large enough
plt.figure(figsize=(18, len(numeric_cols) * 3))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(len(numeric_cols), 1, i)
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(f'Boxplot for {col}', fontsize=14)
    plt.xlabel('')
    plt.grid(axis='x', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

#Isolation Forest Outlier analysis that gives 1 label to outlier and 0 label to data points
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

print("Numeric features:", numeric_cols)
print("Categorical features:", categorical_cols)

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)
    ]
)
isolation_pipeline = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('iso_forest', IsolationForest(
        contamination=0.02,    # Adjust based on expected outlier rate
        n_estimators=300,
        random_state=42,
        max_samples='auto'
    ))
])
# Fit model
isolation_pipeline.fit(df)

# Extract the transformed data to store encoded & scaled version
preprocessed_data = isolation_pipeline.named_steps['preprocess'].transform(df)

# Predict outliers
outlier_flags = isolation_pipeline.named_steps['iso_forest'].predict(preprocessed_data)

# Convert -1 to 1 (outlier), 1 to 0 (normal)
df['iso_outlier_flag'] = np.where(outlier_flags == -1, 1, 0)

print(f"IsolationForest detected {df['iso_outlier_flag'].sum()} outliers.")
plt.figure(figsize=(8,4))
sns.countplot(x=df['iso_outlier_flag'], palette='mako')
plt.title('Outlier Count (IsolationForest with Encoding)')
plt.xlabel('Outlier Flag (1=Outlier)')
plt.ylabel('Count')
plt.show()

#Hybrid Clustering algorithm SCHC :- Spatially Constrained Hierarchical Clustering for geospatial data , categorical data , numerical data .
# !pip install scikit-learn seaborn matplotlib pandas numpy tqdm scipy

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt


# Select relevant columns
numerical_features = [
    'log_revenue', 'latitude', 'longitude', 'distance_to_nearest_hub_km',
    'shops_within_500m', 'shops_within_1000m', 'Demographic_PC_1', 'Demographic_PC_2'
]
categorical_features = [
    'shop_category', 'nearest_hub', 'city', 'pincode'
]


#Standardization & Encoding

scaler = StandardScaler()
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

preprocessor = ColumnTransformer(
    transformers=[
        ('num', scaler, numerical_features),
        ('cat', encoder, categorical_features)
    ],
    remainder='drop'
)

print("Preprocessing dataset...")
X_processed = preprocessor.fit_transform(df)
print(f"Processed feature matrix shape: {X_processed.shape}")

from scipy.spatial.distance import pdist

linkage_methods = ['ward', 'complete', 'average']
metrics = ['euclidean']
n_clusters_list = [2, 3, 4, 5, 6, 7, 8, 9, 10]

results = []

print("\nIterative SCHC Parameter Search...")

for link in tqdm(linkage_methods):
    for metric in metrics:
        if link == 'ward' and metric != 'euclidean':
            continue  # ward only works with Euclidean

        #Compute linkage matrix
        try:
            Z = linkage(X_processed, method=link, metric=metric)
        except MemoryError:
            print(f"Skipping {link}-{metric}: too large for memory.")
            continue

        for k in n_clusters_list:
            labels = fcluster(Z, t=k, criterion='maxclust')
            if len(set(labels)) < 2:
                continue

            try:
                sil = silhouette_score(X_processed, labels)
                db = davies_bouldin_score(X_processed, labels)
                ch = calinski_harabasz_score(X_processed, labels)
            except:
                sil, db, ch = np.nan, np.nan, np.nan

            results.append({
                'Linkage': link,
                'Metric': metric,
                'n_clusters': k,
                'Silhouette': sil,
                'Davies-Bouldin': db,
                'Calinski-Harabasz': ch
            })

results_df = pd.DataFrame(results).dropna().sort_values(by='Silhouette', ascending=False)
print("\nTop 5 SCHC Configurations by Silhouette Score:")
display(results_df.head())
best_params = results_df.iloc[0]
best_linkage = best_params['Linkage']
best_metric = best_params['Metric']
best_k = int(best_params['n_clusters'])

print(f"\nBest SCHC Parameters:\nLinkage: {best_linkage}, Metric: {best_metric}, Clusters: {best_k}")

Z_best = linkage(X_processed, method=best_linkage, metric=best_metric)
labels_best = fcluster(Z_best, t=best_k, criterion='maxclust')
df['SCHC_Cluster'] = labels_best

plt.figure(figsize=(16, 8))
plt.title(f"SCHC Dendrogram ({best_linkage}, {best_metric}) - {best_k} Clusters")
dendrogram(Z_best, truncate_mode='lastp', p=20, leaf_rotation=90., leaf_font_size=8.)
plt.xlabel("Sample Index / Cluster Size")
plt.ylabel("Distance")
plt.show()
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_processed)

df_pca = pd.DataFrame(X_pca, columns=['PCA_1', 'PCA_2'])
df_pca['Cluster'] = df['SCHC_Cluster'].astype(str)

plt.figure(figsize=(10, 8))
sns.scatterplot(data=df_pca, x='PCA_1', y='PCA_2', hue='Cluster', palette='tab10', s=60, alpha=0.8)
plt.title(f"SCHC Clusters (Best Params: {best_linkage}, {best_metric}, k={best_k})")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title='Cluster')
plt.show()
sil = silhouette_score(X_processed, labels_best)
db = davies_bouldin_score(X_processed, labels_best)
ch = calinski_harabasz_score(X_processed, labels_best)

print(f"\nFinal SCHC Evaluation Metrics:")
print(f"Silhouette Score       : {sil:.4f}")
print(f"Davies-Bouldin Index   : {db:.4f}")
print(f"Calinski-Harabasz Score: {ch:.4f}")
num_summary = df.groupby('SCHC_Cluster')[numerical_features].mean()
cat_summary = df.groupby('SCHC_Cluster')[categorical_features].agg(lambda x: x.value_counts().index[0])

print("\nNumeric Summary per Cluster:")
display(num_summary)

print("\nDominant Categories per Cluster:")
display(cat_summary)

df2=df.copy()

!pip install kmodes
!pip install gower

df

# 2)The second hybrid clustering algorithm that we have Gower+HDBSCAN Hybrid clustering alogrithm

# !pip install hdbscan gower scikit-learn seaborn matplotlib tqdm pandas numpy

import pandas as pd
import numpy as np
import hdbscan
import gower
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score


df = pd.read_csv("final_dataset_engineered_multihub.csv")

# We Define numeric and categorical columns
num_cols = ['log_revenue', 'latitude', 'longitude', 'distance_to_nearest_hub_km',
            'shops_within_500m', 'shops_within_1000m', 'Demographic_PC_1', 'Demographic_PC_2']
cat_cols = ['shop_category', 'nearest_hub', 'city', 'pincode']

# Standardize numeric features
scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])

print(" Data standardized successfully.")


# We Compute Gower Distance Matrix (Mixed Data Similarity)

print("Computing Gower distance matrix...")
gower_dist = gower.gower_matrix(df_scaled[cat_cols + num_cols]).astype(np.float64)


print("Gower matrix computed:", gower_dist.shape)


#We Tune HDBSCAN Hyperparameters

param_grid = {
    'min_cluster_size': [20, 40, 60, 80, 100],
    'min_samples': [5, 10, 15],
    'metric': ['precomputed']
}

results = []
print("\n Searching for best HDBSCAN parameters...")
for mcs in tqdm(param_grid['min_cluster_size']):
    for ms in param_grid['min_samples']:
        clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, min_samples=ms, metric='precomputed')
        labels = clusterer.fit_predict(gower_dist)

        # Skip noise-only results
        if len(set(labels)) <= 1:
            continue

        valid_mask = labels != -1
        if valid_mask.sum() < 10:
            continue

        try:
            sil = silhouette_score(1 - gower_dist[valid_mask][:, valid_mask], labels[valid_mask])
            db = davies_bouldin_score(1 - gower_dist[valid_mask][:, valid_mask], labels[valid_mask])
            ch = calinski_harabasz_score(1 - gower_dist[valid_mask][:, valid_mask], labels[valid_mask])
        except Exception:
            sil, db, ch = np.nan, np.nan, np.nan

        results.append({
            'min_cluster_size': mcs,
            'min_samples': ms,
            'Silhouette': sil,
            'DaviesBouldin': db,
            'CalinskiHarabasz': ch,
            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0)
        })

results_df = pd.DataFrame(results).sort_values(by='Silhouette', ascending=False)
print("\nTop HDBSCAN parameter results:")
print(results_df.head())


# We Train Final HDBSCAN with Best Parameters

best_params = results_df.iloc[0]
print("\nBest parameters found for HDBSCAN:")
print(best_params)

best_clusterer = hdbscan.HDBSCAN(
    min_cluster_size=int(best_params['min_cluster_size']),
    min_samples=int(best_params['min_samples']),
    metric='precomputed'
)
labels = best_clusterer.fit_predict(gower_dist)

df['Cluster'] = labels
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
print(f"\n HDBSCAN formed {n_clusters} clusters.")


#We Evaluate Final Model

valid_mask = df['Cluster'] != -1
sil = silhouette_score(1 - gower_dist[valid_mask][:, valid_mask], df['Cluster'][valid_mask])
db = davies_bouldin_score(1 - gower_dist[valid_mask][:, valid_mask], df['Cluster'][valid_mask])
ch = calinski_harabasz_score(1 - gower_dist[valid_mask][:, valid_mask], df['Cluster'][valid_mask])

print(f"\nFinal Evaluation Metrics:")
print(f"Silhouette Score:      {sil:.3f}")
print(f"Daviesâ€“Bouldin Index:  {db:.3f}")
print(f"Calinskiâ€“Harabasz:     {ch:.3f}")


#We perfrom Visualization via PCA (2D Projection)

print("\n Visualizing clusters using PCA...")
pca = PCA(n_components=2, random_state=42)
pca_data = pca.fit_transform(df_scaled[num_cols])
df_vis = pd.DataFrame(pca_data, columns=['PCA1', 'PCA2'])
df_vis['Cluster'] = df['Cluster']

plt.figure(figsize=(10, 7))
sns.scatterplot(data=df_vis, x='PCA1', y='PCA2', hue='Cluster',
                palette='tab20', s=60, alpha=0.9)
plt.title(" Gower + HDBSCAN Hybrid Clustering")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()


# Cluster Summary

print("\nCluster distribution:")
print(df['Cluster'].value_counts())

df4=df
df2=df2.reset_index().rename(columns={'index':'row_id'})
df4=df4.reset_index().rename(columns={'index':'row_id'})
df5=pd.merge(df2, df4[['row_id', 'Cluster']], on='row_id',how='inner')
df5.drop(columns=['row_id'], inplace=True)

df5

df=df5
df.rename(columns={'Cluster':'Grower+HDBSCAN_cluster'}, inplace=True)
df.head()

#Spatial Cluster of Mumbai and nearby region using grower+HDBSCAN
import plotly.express as px
target_cities = df['city'].unique()
df_region = df[df["city"].isin(target_cities)].copy()

print(f"Filtered rows for Mumbai region: {df_region.shape[0]}")

fig_map = px.scatter_mapbox(
    df_region,
    lat="latitude",
    lon="longitude",
    color="Grower+HDBSCAN_cluster",
    hover_data=["city", "shop_category", "log_revenue"],
    zoom=10,
    height=600,
    title="ðŸŒ† Gower + HDBSCAN Hybrid Clusters â€“ Mumbai & Nearby Regions",
    mapbox_style="carto-positron"
)

fig_map.show()

#Spatial Cluster of Mumbia and nearby region using SCHC

!pip install plotly folium pandas numpy


import pandas as pd
import plotly.express as px
import folium
from folium.plugins import MarkerCluster


print("Columns in df_hybrid:", df.columns.tolist())


target_cities = df['city'].unique()
df_region = df[df["city"].isin(target_cities)].copy()

print(f"Filtered rows: {df_region.shape[0]}")
display(df_region.head())


fig_schc = px.scatter_mapbox(
    df_region,
    lat="latitude",
    lon="longitude",
    color="SCHC_Cluster",
    hover_data=["city", "shop_category", "log_revenue"],
    zoom=10,
    height=600,
    title="SCHC Spatial Clusters â€“ Mumbai Region",
    mapbox_style="carto-positron"
)

fig_schc.show()

#Standard Clustering algorithm used here Spectral Clustering Algorithm
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import DBSCAN, SpectralClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np




spatial_features = ['latitude', 'longitude']
numerical_features = ['log_revenue', 'distance_to_nearest_hub_km', 'shops_within_500m',
                      'shops_within_1000m', 'Demographic_PC_1', 'Demographic_PC_2']
categorical_features = ['shop_category', 'nearest_hub', 'city']


preprocessor = ColumnTransformer([
    ('num', StandardScaler(), spatial_features + numerical_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
])


X_processed = preprocessor.fit_transform(df)

print("Data Preprocessed:", X_processed.shape)

from sklearn.neighbors import NearestNeighbors
best_score = -1
best_params = None

for k in range(2, 7):
    for affinity in ['rbf', 'nearest_neighbors']:
        sc = SpectralClustering(n_clusters=k, affinity=affinity, random_state=42, n_jobs=-1)
        labels = sc.fit_predict(X_processed)
        score = silhouette_score(X_processed, labels)
        if score > best_score:
            best_score = score
            best_params = {'n_clusters': k, 'affinity': affinity}

print("Best Spectral Params:", best_params)
print("Best Silhouette:", best_score)


spectral = SpectralClustering(
    n_clusters=best_params['n_clusters'],
    affinity=best_params['affinity'],
    random_state=42,
    n_jobs=-1
)
df['Spectral_cluster'] = spectral.fit_predict(X_processed)

def evaluate_model(X, labels, name):
    mask = labels != -1
    sil = silhouette_score(X[mask], labels[mask])
    ch = calinski_harabasz_score(X[mask], labels[mask])
    dbi = davies_bouldin_score(X[mask], labels[mask])
    print(f"\nðŸ“ˆ {name} Evaluation Metrics:")
    print(f"Silhouette Score: {sil:.3f}")
    print(f"Calinski-Harabasz Score: {ch:.3f}")
    print(f"Davies-Bouldin Score: {dbi:.3f}")
    return sil, ch, dbi
spectral_metrics = evaluate_model(X_processed, df['Spectral_cluster'], "Spectral Clustering")

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_processed)

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=df['Spectral_cluster'], cmap='tab10', s=25)
plt.title("Spectral Clustering Clusters (Mixed Data)")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.show()

#Second Standard Clsutering Algorithm that we have is Kmeans Clustering Algorithm

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns



features = [
    'log_revenue', 'latitude', 'longitude',
    'distance_to_nearest_hub_km',
    'shops_within_500m', 'shops_within_1000m',
    'Demographic_PC_1', 'Demographic_PC_2'
]

X = df[features].copy()


X = X.fillna(X.mean())

#Standardize data for Kmeans
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine optimal K (Elbow Method)
inertia = []
K = range(2, 10)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(7, 4))
plt.plot(K, inertia, 'o-', color='royalblue')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia (Within-Cluster SSE)')
plt.title('Elbow Method for Optimal K')
plt.grid(True)
plt.show()

#Apply KMeans with optimal K
optimal_k = 4  # Change after seeing the elbow
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df['KMeans_Cluster'] = kmeans.fit_predict(X_scaled)

#Evaluation metrics
sil = silhouette_score(X_scaled, df['KMeans_Cluster'])
ch = calinski_harabasz_score(X_scaled, df['KMeans_Cluster'])
db = davies_bouldin_score(X_scaled, df['KMeans_Cluster'])

print("\n Cluster Evaluation Metrics:")
print(f"Silhouette Score: {sil:.3f}")
print(f"Calinski-Harabasz Index: {ch:.3f}")
print(f"Davies-Bouldin Index: {db:.3f}")


pca = PCA(n_components=2)
pca_result = pca.fit_transform(X_scaled)
df['PCA1'], df['PCA2'] = pca_result[:, 0], pca_result[:, 1]

plt.figure(figsize=(8, 6))
sns.scatterplot(
    x='PCA1', y='PCA2', hue='KMeans_Cluster',
    data=df, palette='Set2', alpha=0.8
)
plt.title('K-Means Cluster Visualization (PCA Projection)')
plt.show()


if 'latitude' in df.columns and 'longitude' in df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(
        x='longitude', y='latitude',
        hue='KMeans_Cluster', palette='Set1', data=df, alpha=0.8
    )
    plt.title('Geospatial Distribution of K-Means Clusters')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.show()


if 'SCHC_Cluster' in df.columns:
    comparison = pd.crosstab(df['SCHC_Cluster'], df['KMeans_Cluster'])
    print("\nðŸ“Š Cross-tabulation between SCHC and KMeans clusters:")
    print(comparison)

# Summary statistics per cluster
summary = df.groupby('KMeans_Cluster')[features + ['log_revenue']].mean()
print("\nðŸ“ˆ Cluster Feature Summary:")
print(summary)

df

df.to_csv('outputclustering.csv')

#Data Validation Stage
#Here we perfrom Data VAlidation we perform data validation in 3 ways geopspatial data validation , external data validation , meaning data validation
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings


warnings.filterwarnings('ignore')
sns.set(style="whitegrid")


cluster_file = '/content/outputclustering.csv'
original_data_file = '/content/final_dataset1 (1).csv'

try:

    df_clustered = pd.read_csv(cluster_file)
    print(f"Loaded clustered data from '{cluster_file}'. Shape: {df_clustered.shape}")


    df_original = pd.read_csv(original_data_file)
    print(f"Loaded original data from '{original_data_file}'. Shape: {df_original.shape}")

except FileNotFoundError as e:
    print(f"Error loading files: {e}. Please ensure both CSVs are uploaded.")
    raise


original_demographic_features = [
    'Unnamed: 0', # This is the merge key
    'TOT_P',
    'Total_literacy_rate',
    'ill_rate',
    'childreen_population_rate',
    'Cultivators_pulation_rate',
    'agricultural_population_rate',
    'household_industry_population_rate',
    'Other_Worker_population_rate',
    'marginial_workers_population_rate'
]
df_original_subset = df_original[original_demographic_features]


df_validation = pd.merge(df_clustered, df_original_subset, on='Unnamed: 0', how='left')

print(f"Merged validation DataFrame shape: {df_validation.shape}")


cluster_cols = {
    'Grower+HDBSCAN':'Grower+HDBSCAN_cluster',
    'SCHC': 'SCHC_Cluster'
}


key_engineered_features = ['log_revenue', 'distance_to_nearest_hub_km', 'shops_within_1000m']
key_categorical_features = ['shop_category', 'nearest_hub']


print("\n--- 1. Geospatial Validation (Creating Maps) ---")

gdf = gpd.GeoDataFrame(
    df_validation,
    geometry=gpd.points_from_xy(df_validation.longitude, df_validation.latitude),
    crs="EPSG:4326"
)

fig, axes = plt.subplots(1, 2, figsize=(27, 8))
fig.suptitle('Geospatial Validation: Comparing Cluster Maps', fontsize=20, y=1.02)
gdf.plot(ax=axes[0], column=cluster_cols['Grower+HDBSCAN'], legend=True, markersize=5, cmap='tab20', categorical=True)
axes[0].set_title('Grower+HDBSCAN')
gdf.plot(ax=axes[1], column=cluster_cols['SCHC'], legend=True, markersize=5, cmap='Dark2', categorical=True)
axes[1].set_title('SCHC Clusters')
plt.tight_layout()
plt.savefig('validation_geospatial_comparison.png')
print("Saved geospatial comparison map: 'validation_geospatial_comparison.png'")

# External Validation (The "Profitability" Test) using reveuen as the attribute
print("\n--- 2. External Validation (Revenue Box Plots) ---")
fig, axes = plt.subplots(1, 2, figsize=(24, 7))
fig.suptitle('External Validation: log_revenue by Cluster', fontsize=20, y=1.02)
sns.boxplot(ax=axes[0], data=df_validation, x=cluster_cols['Grower+HDBSCAN'], y='log_revenue', palette='tab20')
axes[0].set_title('Grower+HDBSCAN: Revenue vs. Cluster')
axes[0].tick_params(axis='x', rotation=90)
sns.boxplot(ax=axes[1], data=df_validation, x=cluster_cols['SCHC'], y='log_revenue', palette='Dark2')
axes[1].set_title('SCHC: Revenue vs. Cluster')
plt.tight_layout()
plt.savefig('validation_external_revenue_comparison.png')
print("Saved revenue boxplot comparison: 'validation_external_revenue_comparison.png'")

#Profiling of clases to understand there meaning what these class provide
import pandas as pd
import numpy as np


all_possible_numerical = [
    'revenue', 'Total_literacy_rate', 'ill_rate', 'childreen_population_rate',
    'SC_ST_RATE', 'TOT_P', 'Cultivators_pulation_rate', 'agricultural_population_rate',
    'household_industry_population_rate', 'Other_Worker_population_rate',
    'marginial_workers_population_rate', 'marginial_workers_lessthan6months_population_rate',
    'total_working_population_rate'
]

all_possible_categorical = ['shop_category', 'city']

numerical_features = [col for col in all_possible_numerical if col in df_validation.columns]
categorical_features = [col for col in all_possible_categorical if col in df_validation.columns]

print(f"Using numerical features: {numerical_features}")
print(f"Using categorical features: {categorical_features}")


def label_value(value, mean, std):
    """Label feature as High / Medium / Low relative to global mean Â± 0.5Ïƒ."""
    if value >= mean + 0.5 * std:
        return "High"
    elif value <= mean - 0.5 * std:
        return "Low"
    else:
        return "Medium"

def assign_cluster_profiles(df, cluster_col, algo_name):
    print(f"\n Profiling clusters for {algo_name}\n")

    if cluster_col not in df.columns:
        print(f"Column '{cluster_col}' not found. Skipping.")
        return pd.DataFrame()


    global_means = df[numerical_features].mean()
    global_stds = df[numerical_features].std()


    cluster_means = df.groupby(cluster_col)[numerical_features].mean().reset_index()
    cluster_modes = (
        df.groupby(cluster_col)[categorical_features]
        .agg(lambda x: x.mode()[0] if not x.mode().empty else None)
        .reset_index()
    )

    cluster_summary = pd.merge(cluster_means, cluster_modes, on=cluster_col, how="left")

    profiles = []
    for _, row in cluster_summary.iterrows():
        desc_parts = []


        for key_feat in ['revenue', 'Total_literacy_rate', 'agricultural_population_rate']:
            if key_feat in row:
                lbl = label_value(row[key_feat], global_means[key_feat], global_stds[key_feat])
                desc_parts.append(f"{lbl} {key_feat.replace('_', ' ')}")


        if 'shop_category' in row: desc_parts.append(f"Dominant shop type: {row['shop_category']}")
        if 'city' in row: desc_parts.append(f"Major city: {row['city']}")

        profiles.append({
            "Cluster": row[cluster_col],
            "Profile": "; ".join(desc_parts)
        })

    profile_df = pd.DataFrame(profiles)
    print(profile_df)
    return profile_df


profile_gower = assign_cluster_profiles(df_validation, 'Grower+HDBSCAN_cluster', 'Grower+HDBSCAN_cluster')
profile_schc  = assign_cluster_profiles(df_validation, 'SCHC_Cluster', 'SCHC')


if not profile_gower.empty:
    df_validation = df_validation.merge(
        profile_gower, how='left', left_on='Grower+HDBSCAN_cluster', right_on='Cluster', suffixes=('', '_GowerProfile')
    )

if not profile_schc.empty:
    df_validation = df_validation.merge(
        profile_schc, how='left', left_on='SCHC_Cluster', right_on='Cluster', suffixes=('', '_SCHCProfile')
    )

display(df_validation[['Grower+HDBSCAN_cluster', 'Profile', 'SCHC_Cluster', 'Profile_SCHCProfile']].head())

#Class Prediction using by trainig the model on text dataset and and testing on test dataset using shop category , revenue
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils.class_weight import compute_class_weight



# Replace with your actual file paths
profiles_path = "/content/final_dataset1 (1).csv"
clusters_path = "/content/outputclustering.csv"

profiles = pd.read_csv(profiles_path)
clusters = pd.read_csv(clusters_path)

print(f"Profiles shape: {profiles.shape}")
print(f"Clusters shape: {clusters.shape}")

# Try merging using common columns automatically
common_cols = list(set(profiles.columns) & set(clusters.columns))
merged_df = pd.merge(profiles, clusters, on=common_cols, how="inner")
print(f"Merged DataFrame shape: {merged_df.shape}")


shop_col = "shop_category"
rev_col = "revenue"

# Detect cluster columns dynamically
cluster_cols = [col for col in merged_df.columns if "cluster" in col.lower()]
print(f"Cluster columns found: {cluster_cols}")

# Keep only required columns
feature_cols = [shop_col, rev_col]

# Drop rows with missing data
merged_df = merged_df.dropna(subset=feature_cols + cluster_cols)


le_shop = LabelEncoder()
merged_df["shop_category_encoded"] = le_shop.fit_transform(merged_df[shop_col])

X = merged_df[["shop_category_encoded", rev_col]]


models = {}

for cluster_col in cluster_cols:
  if cluster_col!='KMeans_Cluster':
      print(f"\n---  Training for {cluster_col} ---")
      y = merged_df[cluster_col].astype(str)

      # Remove clusters with too few samples
      cluster_counts = y.value_counts()
      rare_clusters = cluster_counts[cluster_counts < 10].index
      y = y[~y.isin(rare_clusters)]
      X_cluster = X.loc[y.index]

      if len(y.unique()) < 2:
          print(f"Skipping {cluster_col} (not enough labels)")
          continue

      X_train, X_test, y_train, y_test = train_test_split(
          X_cluster, y, test_size=0.25, random_state=42, stratify=y
      )


      class_weights = dict(zip(
          np.unique(y_train),
          compute_class_weight("balanced", classes=np.unique(y_train), y=y_train)
      ))

      model = RandomForestClassifier(
          n_estimators=250,
          max_depth=8,
          min_samples_leaf=3,
          class_weight=class_weights,
          random_state=42
      )
      model.fit(X_train, y_train)

      y_pred = model.predict(X_test)
      acc = accuracy_score(y_test, y_pred)

      print(f" Accuracy: {acc:.3f}")
      print(classification_report(y_test, y_pred))

      # Cross-validation for stability
      cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
      cv_f1 = cross_val_score(model, X_cluster, y, cv=cv, scoring="f1_macro").mean()
      print(f"Cross-validated F1 Score: {cv_f1:.3f}")

      models[cluster_col] = model

print("\nAll models trained successfully!")


def predict_cluster(cluster_type, shop_name, revenue_val):
    if cluster_type not in models:
        print(f" Model not trained for {cluster_type}")
        return

    model = models[cluster_type]
    try:
        shop_encoded = le_shop.transform([shop_name])[0]
    except ValueError:
        print(f"Invalid shop name! Available: {list(le_shop.classes_)}")
        return

    new_data = pd.DataFrame([[shop_encoded, revenue_val]],
                            columns=["shop_category_encoded", rev_col])
    pred = model.predict(new_data)[0]
    proba = model.predict_proba(new_data)[0]

    cluster_labels = model.classes_
    top3_idx = np.argsort(proba)[::-1][:2]

    print(f"\n Predicted Cluster for {cluster_type}: {pred}")
    print("Top 3 Cluster Probabilities:")
    for i in top3_idx:
        print(f"  â€¢ Cluster {cluster_labels[i]} â†’ {proba[i]*100:.2f}%")

predict_cluster("Grower+HDBSCAN_cluster", "Food & Grocery",2000000)

